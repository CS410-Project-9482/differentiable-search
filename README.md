# A Two-Stage Search Engine with Differentiable Learning-to-Rank

## 1. Project Overview
This project implements a **Neural Re-Ranking Architecture** designed to optimize Information Retrieval metrics (nDCG) directly via gradient descent.

*   **Stage 1 (Retrieval):** Uses `Pyserini` (BM25) to fetch high-recall candidates from the **AP News** dataset.
*   **Stage 2 (Re-Ranking):** Applies a Neural Linear Scorer trained using **LapSum** (a differentiable sorting operator).

### Scientific Context
In the **CS410 MP1 Assignment**, we established the baseline performance for the `apnews` dataset using static heuristic tuning.
*   **MP1 Baseline:** The optimal static BM25 parameters ($k1=2.6, b=0.9$) achieved an **nDCG@10 of 0.4**.
*   **Project Goal:** To demonstrate that a differentiable sorting operator allows a model to **learn** optimal ranking weights end-to-end, aiming to surpass the static baseline without manual parameter grid search.
*   **Result:** The trained model converges with an nDCG@10 of **>0.50**, confirming the efficacy of the differentiable ranking approach.

## 2. Project Structure
The submission contains the following artifacts:

*   **`run_course_project.py`**: **(Main Entry Point)** The primary executable. It downloads the AP News data, indexes it via Pyserini, and trains the neural re-ranker.
*   **`lapsum_pytorch/`**: The core implementation of the differentiable `LogSoftTopK` operator (Autograd function).
*   **`tests/`**: Unit tests (`test_gradients.py`) verifying the mathematical correctness of the gradient flow through the sorting operator.
*   **`utils/`**: Helper functions for Pyserini retrieval and data loading.
*   **`results/`**: Contains the execution logs and training curve plot generated by the script.
*   **`CS410_Project_Report.pdf`**: The consolidated report documenting the execution results and training dynamics.

## 3. Installation & Usage

### Prerequisites
*   **Python 3.10+**
*   **Java 21 (JDK)** (Strictly required for Pyserini/Lucene operations).

### Environment Setup (Conda Recommended)
```bash
conda create -n course_project_env -c conda-forge python=3.10 openjdk=21
conda activate course_project_env
```

### Execution Steps

1.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

2.  **Verify Mathematical Logic (Optional):**
    Run the gradient check to confirm the sorting operator is differentiable.
    ```bash
    python tests/test_gradients.py
    ```

3.  **Run the Training Pipeline:**
    This script handles data ingestion, indexing, and training.
    ```bash
    python run_course_project.py
    ```
    *   **Console Output:** Monitor the `nDCG@10` metric in the terminal. It should rise to convergence.
    *   **Generated Artifacts:** The script will output the raw training data to:
        *   `results/execution.log`
        *   `results/training_curve.png`


## 5. References
*   *\"LapSum: One Method to Differentiate Them All\"* (ICML 2025) - https://arxiv.org/abs/2503.06242 - [Original Paper Code](https://github.com/gmum/LapSum)